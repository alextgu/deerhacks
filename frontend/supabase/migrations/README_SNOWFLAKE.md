# Snowflake setup for the Unified Matching Layer

Run in **Snowflake** (not Supabase). Use the same database/schema as in your env vars (`SNOWFLAKE_DATABASE`, `SNOWFLAKE_SCHEMA`).

## 1. Table: raw_user_onboarding

Holds the PII-scrubbed Takeout corpus. The backend `/extract` endpoint writes here; Snowflake then generates the 768-dim embedding from this text.

```sql
CREATE TABLE IF NOT EXISTS raw_user_onboarding (
  auth0_id       VARCHAR(255) NOT NULL PRIMARY KEY,
  cleaned_corpus TEXT         NOT NULL,
  updated_at     TIMESTAMP_NTZ DEFAULT CURRENT_TIMESTAMP()
);
```

## 2. Table: user_archetypes

Stores the 768-dim embeddings (generated by Snowflake Cortex from `raw_user_onboarding.cleaned_corpus`). This is the **single source of truth** for all matching (backend and frontend).

```sql
CREATE TABLE IF NOT EXISTS user_archetypes (
  auth0_id       VARCHAR(255) NOT NULL,
  server_id      VARCHAR(255) NOT NULL,
  archetype_vector VECTOR(FLOAT, 768) NOT NULL,
  archetype_label VARCHAR(255) DEFAULT 'Analyzed Persona',
  is_flagged     BOOLEAN DEFAULT FALSE,
  updated_at     TIMESTAMP_NTZ DEFAULT CURRENT_TIMESTAMP(),
  PRIMARY KEY (auth0_id, server_id)
);
```

## 3. Generating embeddings (run after corpus upload)

The backend `embed_and_upsert_archetype()` calls this automatically. For manual / batch runs:

```sql
MERGE INTO user_archetypes tgt
USING (
  SELECT
    auth0_id,
    'general' AS server_id,
    SNOWFLAKE.CORTEX.EMBED_TEXT_768('snowflake-arctic-embed-m', cleaned_corpus) AS archetype_vector,
    'Analyzed Persona' AS archetype_label
  FROM raw_user_onboarding
  WHERE auth0_id = :user_id
) src
ON tgt.auth0_id = src.auth0_id AND tgt.server_id = src.server_id
WHEN MATCHED THEN UPDATE SET
  tgt.archetype_vector = src.archetype_vector,
  tgt.updated_at = CURRENT_TIMESTAMP()
WHEN NOT MATCHED THEN INSERT (user_id, server_id, archetype_vector, archetype_label)
  VALUES (src.auth0_id, src.server_id, src.archetype_vector, src.archetype_label);
```

## 4. Cortex Search Service (ARCHETYPE_MATCH_SERVICE)

Both the backend and frontend use this single Cortex Search service for matching. Set `CORTEX_SEARCH_SERVICE_NAME=ARCHETYPE_MATCH_SERVICE` in your env.

```sql
CREATE OR REPLACE CORTEX SEARCH SERVICE ARCHETYPE_MATCH_SERVICE
  VECTOR INDEXES archetype_vector
  ATTRIBUTES server_id, is_flagged
  WAREHOUSE = MIRROR_WH
  TARGET_LAG = '1 hour'
  AS SELECT user_id, server_id, archetype_vector, is_flagged, updated_at
  FROM user_archetypes;
```

After the service is created:

- **multi_index_query**: vector search on the `embedding` column (override with `CORTEX_SEARCH_VECTOR_INDEX` if different).
- **filter**: `{"@and": [{"@eq": {"server_id": "<serverId>"}}, {"@eq": {"is_flagged": false}}]}` to scope by server and exclude flagged users.
